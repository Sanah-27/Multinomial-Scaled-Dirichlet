{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cb0440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (pytensor.configdefaults): g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n",
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix dimensions: 38658 rows, 15000 columns\n"
     ]
    }
   ],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Load the Bag of Words (BoW) matrix (already sparse)\n",
    "X_text = sp.load_npz('C:/Users/Sanah Iftikhar/Downloads/archive/X_text.npz')\n",
    "\n",
    "# Define the number of clusters or components for the MBL model\n",
    "n_clusters = 100  # Adjust based on your needs\n",
    "\n",
    "# Convert sparse matrix to dense (for further manipulation if needed)\n",
    "X_text_dense = X_text.toarray()\n",
    "\n",
    "# Get dimensions of the feature matrix (NX: number of samples, DX: number of features)\n",
    "NX, DX = X_text_dense.shape\n",
    "print(f\"Feature matrix dimensions: {NX} rows, {DX} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4778a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered feature matrix dimensions: 38658 rows, 9127 columns\n"
     ]
    }
   ],
   "source": [
    "# Compute feature sums\n",
    "feature_sums = np.sum(X_text_dense, axis=0)\n",
    "\n",
    "# Apply a threshold to remove low-frequency features\n",
    "threshold = 10**2  # Increase the threshold\n",
    "high_freq_features = feature_sums > threshold\n",
    "\n",
    "# Filter the feature matrix\n",
    "X_text_preprocessed = X_text_dense[:, high_freq_features]\n",
    "\n",
    "# Get updated dimensions after filtering\n",
    "NX, DX_filtered = X_text_preprocessed.shape\n",
    "print(f\"Filtered feature matrix dimensions: {NX} rows, {DX_filtered} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5fa113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dense array\n",
    "np.save('C:/Users/Sanah Iftikhar/Downloads/archive/X_text_preprocessed.npy', X_text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "035304b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  label\n",
      "0  Donald Trump just couldn t wish all Americans ...      0\n",
      "1  House Intelligence Committee Chairman Devin Nu...      0\n",
      "2  On Friday, it was revealed that former Milwauk...      0\n",
      "3  On Christmas day, Donald Trump announced that ...      0\n",
      "4  Pope Francis used his annual Christmas Day mes...      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined DataFrame from the CSV file\n",
    "news = pd.read_csv(r'C:\\Users\\Sanah Iftikhar\\Downloads\\archive\\news_combined.csv')\n",
    "\n",
    "# Check the DataFrame to ensure it's loaded correctly\n",
    "print(news.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba8e56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters (or categories: fake = 0, true = 1)\n",
    "K = 2  # Since we have 2 classes (fake and true)\n",
    "\n",
    "# Assign labels (0 for fake, 1 for true)\n",
    "data_id = news['label'].values  # Use the 'label' column directly\n",
    "\n",
    "# Your preprocessed feature matrix\n",
    "data = np.load('X_text_preprocessed.npy')  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7939bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    21192\n",
      "0    17466\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#REMOVABLE!\n",
    "# Count the occurrences of each label in the 'label' column\n",
    "label_counts = news['label'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af2d2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import numpy as np\n",
    "\n",
    "def polya_moment_match(data):\n",
    "    # Step 1: Calculate the sums of the features\n",
    "    feature_sums = np.sum(data, axis=0)  # Sum each feature across all samples\n",
    "    \n",
    "    # Step 2: Apply MLE directly by adding a small constant\n",
    "    alpha_params = feature_sums + 1e-5  # Small constant to avoid zero division\n",
    "\n",
    "    return alpha_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1d83a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSDMinitial(data, data_id, K):\n",
    "    NX, DX = data.shape\n",
    "    num_in_class = np.zeros(K)\n",
    "    alpha0 = np.zeros((K, DX))\n",
    "    \n",
    "    # K-means clustering or predefined labels\n",
    "    for j in range(K):\n",
    "        idtmp = np.where(data_id == j)[0]  # Get indices for class j\n",
    "        num_in_class[j] = len(idtmp)\n",
    "        data_c = data[idtmp, :]\n",
    "        alpha0[j, :] = polya_moment_match(data_c)  # Compute alpha parameters using MLE\n",
    "    \n",
    "    beta0 = np.ones((K, DX))  # Initialize beta parameters\n",
    "    Priors = num_in_class / NX  # Calculate priors\n",
    "\n",
    "    return Priors, alpha0, beta0, num_in_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5197fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial model fitting\n",
    "Priors, alpha0, beta0, num_in_class = MSDMinitial(data, data_id, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1f5e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "\n",
    "def msdpdfln(X, alpha, beta):\n",
    "    \"\"\"\n",
    "    Multinomial Scaled Dirichlet log probability density function (pdf)\n",
    "    \n",
    "    Parameters:\n",
    "    X (ndarray): n-by-d count matrix (your data)\n",
    "    alpha (ndarray): k-by-d parameter values (alpha parameters)\n",
    "    beta (ndarray): k-by-d parameter values (beta parameters)\n",
    "    \n",
    "    Returns:\n",
    "    logl (ndarray): n-by-1 log probability densities for each row of X\n",
    "    \"\"\"\n",
    "    n = np.sum(X, axis=1)  # Sum along rows\n",
    "    alpha_rowsums = np.sum(alpha, axis=1) if alpha.ndim > 1 else np.sum(alpha)\n",
    "\n",
    "    logl = gammaln(n + 1) - np.sum(gammaln(X + 1), axis=1)\n",
    "    logl += np.sum(gammaln(X + alpha), axis=1) - np.sum(gammaln(alpha), axis=1)\n",
    "    logl -= np.sum(X * np.log(beta), axis=1)\n",
    "    logl += gammaln(alpha_rowsums) - gammaln(alpha_rowsums + n)\n",
    "\n",
    "    return logl\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
